send (send (send *ri* :get-auto-stabilizer-param) :default_zmp_offsets) :methods
send (send (send *ri* :get-auto-stabilizer-param) :default_zmp_offsets) :slots

send *ri* :start-auto-balancer
send *ri* :set-auto-stabilizer-param :default-zmp-offsets '(#f(-0.02 0.02) #f(-0.02 -0.02)) ;;rleg lleg
send *ri* :stop-auto-balancer

progn (send *jaxon_red* :reset-pose) (send *jaxon_red* :move-centroid-on-foot :both (list :lleg :rleg))
progn (send *jaxon_red* :lleg :move-end-pos #f(100 0 0) :world) (send *jaxon_red* :rleg :move-end-pos #f(-100 0 0) :world) (send *jaxon_red* :move-centroid-on-foot :both (list :lleg :rleg))



;;左足のかかと、右足のつま先を軸にIK解いて回転

arai@arai-desktop ~/ros/coral_ws/src/jsk_recognition/jsk_perception/sample (master) (ROBOT=)
 $ roslaunch realsense2_camera rs_d435_and_t265.launch rviz:=false USE_T265:=false color_width:=640 color_height:=480

(base) arai@arai-desktop ~/ros/coral_ws/src/realsense-ros/realsense2_camera/launch (l515_and_t265) (ROBOT=)
 $ rqt_image_view

(py3.8) arai@arai-desktop ~/ros/coral_ws/src/coral_usb_ros/launch (skelton) (ROBOT=)
 $ roslaunch edgetpu_human_pose_estimator.launch INPUT_IMAGE:=/rs_l515/color/image_raw

arai@arai-desktop ~/ros/coral_ws/src/jsk_recognition/jsk_perception/launch (skelton-with-depth) (ROBOT=)
 $ roslaunch skeleton_with_depth.launch 

rostopic echo skeleton_with_depth/output/skeletons

arai@arai-desktop ~/ros/coral_ws/src/jsk_recognition/jsk_perception/launch (skelton-with-depth) (ROBOT=)
 $ rosrun image_view image_view image:=/edgetpu_human_pose_estimator/output/image

roslaunch realsense2_camera rs_d435_and_t265.launch rviz:=false USE_T265:=false color_width:=640 color_height:=480 
roslaunch coral_usb edgetpu_human_pose_estimator.launch INPUT_IMAGE:=/rs_l515/color/image_raw 
roslaunch jsk_perception skeleton_with_depth.launch
rosrun image_view image_view image:=/edgetpu_human_pose_estimator/output/image
jsk_recognition branch skelton-with-depth
coral_usb_ros branch skelton->iory/human-pose-estimater
realsense2_camera branch l515_and_t265

load "recognition.l"
send *people-pose* :ros-init
send *people-pose* :ros-motion

;;realsensexyz
x左y下z前
;;JAXONのIK
larm-locate #f(611 175 43) -> #f(811 125 43)

ssh leus@jaxonred ;;password leus
roscore

ssh leus@jaxonredvision ;;password leus 
./start-arai.sh

rossetjaxon_red ;;別端末 別端末で開くときは毎回する
rqt_image_view ;;/edgetpu_human_pose_estimator/output/image

anaconda pip install pyrealsense2==2.54.2.5684

vision
rosbag play ~~
roslaunch coral_usb
roslaunch jsk_perception


send *ri* :start-log
save-log

(dotimes (i 1) (send *robot* :reset-pose) (lleg-foot :run nil)(start-pos :run nil) (send *irtviewer* :draw-objects) (wake))

car (send (send *robot* :rarm :end-coords :copy-worldcoords) :rpy-angle)

jaxongoogle leus1783

setq a (gnuplot)
setq b listをつくる
send a :draw b

send *ri* :set-interpolation-mode :linear

rosbag playしてrvizに反映
rosrun rqt_plot rqt_plot topic選んで値をグラフにする
rqt_plotでもいける

send *ri* :state :reference-force-vector
send *ri* :state :reference-moment-vector

M-x xml-mode

(list (apply #'concatenate cons (flatten (mapcar #'(lambda (f m) (list f m)) (send *ri* :state :reference-force-vector) (send *ri* :state :reference-moment-vector)))))

send *robot* :rarm :end-coords :draw-on :size 100 :flush t

pkill -9 choreonoid

hand,legsensorを抜いて起動し直したときはrmfoし直す
